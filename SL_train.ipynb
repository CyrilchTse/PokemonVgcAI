{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SL_train.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNYyFlcyTAwvsG2ILm7Liyd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch \n",
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "ARIYksjZVFOm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter, deque\n",
        "import pandas as pd\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from tqdm.notebook import tqdm\n",
        "import io\n",
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "ymhVIXKSSeKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\connecting Google drive for data"
      ],
      "metadata": {
        "id": "Taqo-eObLQyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "NUt0tAFwLXE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link1 = '1DK0U6_0v4N-v70_zUjei0B4pVOgKceuo' #download data\n",
        "downloaded = drive.CreateFile({'id':link1}) \n",
        "downloaded.GetContentFile('merged_data.csv')  \n",
        "df_train = pd.read_csv('merged_data.csv', header=None) # training data"
      ],
      "metadata": {
        "id": "24ft_xx0MOBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link2 = '1zh76HFJ7j5ZqWg4Rqe1ClM7107jDaI0M' #download data\n",
        "downloaded = drive.CreateFile({'id':link2}) \n",
        "downloaded.GetContentFile('merged_dev.csv')  \n",
        "df_test = pd.read_csv('merged_dev.csv', header=None) # dev data"
      ],
      "metadata": {
        "id": "_tzY6FdSSSZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!#unzip /content/out.zip -d /content/train/"
      ],
      "metadata": {
        "id": "6WjhxrvHSnu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data checking"
      ],
      "metadata": {
        "id": "7MqZXiK3MMzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()\n"
      ],
      "metadata": {
        "id": "DH2wMH529juY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build ANN"
      ],
      "metadata": {
        "id": "_oUQvF3oMpkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,filenames):\n",
        "      df = pd.read_csv(filenames)\n",
        "      self.features = df.iloc[:,:-4]\n",
        "      self.label = df.iloc[:,-3:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      features = self.features[index]\n",
        "      label = self.label[index]\n",
        "      return file#, labels\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.features)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TN_S3hW1Mq6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/train/merged_train.csv'\n",
        "dev = '/content/dev/merged_dev.csv'\n",
        "\n",
        "train_dataset = Dataset(train_path)\n",
        "dev_dataset = Dataset(dev)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "print(len(train_loader))\n",
        "#print(len(dev_loader))"
      ],
      "metadata": {
        "id": "d7Qp5_WzNg_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SLnetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SLnetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.nn.Linear(356, 100)\n",
        "        x = self.linear_relu_stack()\n",
        "        x = self.Softmax(dim = 8)\n",
        "        return x"
      ],
      "metadata": {
        "id": "15BGGuUeNkB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = SLnetwork()\n",
        "# Define the cost function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer, learning rate \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "YUvq7orXNpn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2): \n",
        "      for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.squeeze(1))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cost = loss.item()\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch:' + str(epoch) + \", Iteration: \" + str(i) \n",
        "                  + \", training cost = \" + str(cost))"
      ],
      "metadata": {
        "id": "_fGaK7jrN0W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(loader):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "  \n",
        "    all_record = []\n",
        "    all_preds = []\n",
        "    all_label = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            record, label = data\n",
        "            outputs = model(record)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.squeeze()).sum().item()\n",
        "            \n",
        "            all_record.append(record)\n",
        "            all_preds.append(predicted.numpy())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return 100 * correct / total, all_record, all_preds, all_labels"
      ],
      "metadata": {
        "id": "8Th30syqN1Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy, _ , _, _ = calculate_accuracy(train_loader)\n",
        "test_accuracy, record, preds, labels = calculate_accuracy(test_loader)\n",
        "\n",
        "print('Train accuracy: %f' % train_accuracy)\n",
        "print('Test accuracy: %f' % test_accuracy)\n",
        "\n",
        "record = np.concatenate(record, axis=0)\n",
        "preds = np.concatenate(preds, axis=0)\n",
        "labels = np.squeeze(np.concatenate(labels, axis=0))"
      ],
      "metadata": {
        "id": "2QcEkUS0OBSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}